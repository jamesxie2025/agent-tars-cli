version: '3.8'

services:
  agent-tars:
    image: ghcr.io/jamesxie2025/agent-tars-cli:latest
    platform: linux/amd64
    container_name: agent-tars
    restart: unless-stopped
    
    ports:
      - "${PORT:-8080}:8080"
    
    volumes:
      - ./data:/app/data
      - ./cache:/app/cache
      - ./generated:/app/generated
      - ./workspace:/app/workspace
      - ./mcp-config.ts:/app/mcp-config.ts:ro
      - ./agent-config.ts:/app/agent-config.ts:ro

    env_file:
      - .env

    environment:
      - NODE_ENV=production
      - TZ=Asia/Shanghai
      # Model configuration - these will be read from .env file
      # You can override them here if needed
      - TARS_MODEL_PROVIDER=${TARS_MODEL_PROVIDER}
      - TARS_MODEL_NAME=${TARS_MODEL_NAME}
      - TARS_MODEL_BASE_URL=${TARS_MODEL_BASE_URL}
      - TARS_MODEL_API_KEY=${TARS_MODEL_API_KEY}
    
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
